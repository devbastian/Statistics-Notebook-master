---
title: "Skills Quiz: Logistic Regression"
output: 
  html_document:
    theme: cerulean
    code_folding: hide
    toc: true
    toc_float: true
---


## Instructions

Use this file to keep a record of your work as you complete the "Skills Quiz: Logistic Regression" assignment in Canvas.


```{r, message=FALSE, warning=FALSE}
pacman::p_load(readr, haven, readxl, downloader, tidyverse, ggbeeswarm, mosaic, stringr, pander, DT, ggplot2, alr3, foreign, measurements, nycflights13, Lahman, blscrapeR, lubridate, riem, ggthemes, ggrepel, ResourceSelection)
```


----

<!-- Note: The {} after each Problem and Part header allows you to keep track of what work you have completed. Write something like {Done} once you complete each problem and your html file will then show you a nice summary of what you have "done" already. -->


## Problem 1 {Done}

Use the KidsFeet data set from library(mosaic) to practice fitting a logistic regression.

### Part (a)

Is birthmonth enough information to predict the birthyear of a fourth grade student? Fit a logistic regression of birthyear == 88 on the birthmonth of a child to find out.

Enter the values of your estimates for $\beta_0$ and $\beta_1$ in the logistic model:

$$
  P(\text{birthyear}_i = 88 \, | \, \text{birthmonth}_i) = \frac{e^{\beta_0 + \beta_1 \text{birthmonth}_i}}{1 + e^{\beta_0 + \beta_1 \text{birthmonth}_i}}
$$

<div class="YourAnswer">

```{r}
Kid.glm <- glm(birthyear == 88 ~ birthmonth, data = KidsFeet, family = binomial)
summary(Kid.glm)
```

Estimate of $\beta_0$: 9.9665

Estimate of $\beta_1$: -0.9992

</div>


### Part (b)

Fit a new model using only the width of the child's foot instead of birthmonth to predict the birthyear of the child. Is foot width a better or worse predictor of the birthyear of the child than birthmonth?

State the AIC, null deviance, and residual deviance of both logistic regression models.

<div class="YourAnswer">

```{r}
Kid.glm2 <- glm(birthyear == 88 ~ width, data = KidsFeet, family = binomial)
summary(Kid.glm2)
```

| Predictor Variable | AIC | Null Deviance | Residual Deviance| $p$-value |
|------------|------------|------------|------------|------------|
| birthmonth | 20.921 | 36.708 | 16.921 | 0.01198 |
| foot width | 37.877 | 36.708 | 33.877 | 0.1179 |

</div>


### Part (c)

Use the better model of the two models from parts (a) and (b) to predict the probability that a fourth grade boy (from the same era as the students in this data) was born in 88 given they had a foot width of 9.5 cm, a foot length of 24, and was born in May. (This is more information than you need to make the prediction.)

<div class="YourAnswer">

```{r}
Kid2 <- KidsFeet %>% 
  filter(sex == "B")

Kid.glm3 <- glm(birthyear == 88 ~ birthmonth, data = KidsFeet, family = binomial)

predict(Kid.glm3, data.frame(birthmonth = 5, sex = "B"), type = "response")
#View(KidsFeet)
```

Predicted probability: `0.9931076`

</div>

### Part (d)

Interpret the effect on the odds that the slope term from the model used in Part (c) has on the odds of a fourth grade child being born in 88.

<div class="YourAnswer">

```{r}
summary(glm(birthyear == 88 ~ birthmonth, data = KidsFeet, family = binomial))

exp(-0.9992)
```

"The odds of the child being born in 88 decrease by 63% for each one month increase in the birthmonth (1 being January and 12 being December). In other words, a child born in February has an odds of being born in 88 that is 63% less than a child that was born in January. And the same is true for any two consecutive months."

</div>



## Problem 2 {Done}

Consider the `?RailTrail` data set in library(mosaicData). As shown by the boxplot below, the number of users on the rail trail seems to be influenced by whether or not there was any precipitation on that day.

```{r}
boxplot(volume ~ precip>0, data=RailTrail, col="skyblue", xlab="90 Days of Records from April to November", ylab="Total Number of Users on the Trail that Day", main="Rain Seems to Reduce the Number of Users", names = c("Days with No Rain", "Rainy Days"))
```

The goal of this question is to identify a logistic regression model that could be used to predict if there will be rain on a given day or not.

Run the following commands to reduce the data to variables of interest for this problem.

```{r}
#library(dplyr)
RT <- RailTrail %>% 
  mutate(precipOccurred = (precip > 0)) %>%
  select(precipOccurred, lowtemp, spring, summer, fall, cloudcover, weekday)
```


### Part (a)

Create a pairs plot that shows how well each variable in the RT data set can explain whether or not precipitation occurred on a given day. State which variables seem to be the strongest predictors of precipitation occurring.

(Note that a major limitation of this data is that all measurements are on the "day of" the precipitation. The data could potentially be more insightful if it contained the temperature or other information for the day prior, or two days prior, or so on... as well.)


<div class="YourAnswer">

```{r}
#Hint: pairs(..., pch=16, col=rgb(.2,.2,.2,.1))
#Or: pairs(..., pch=16, col=as.factor(yourData$yourYvariable))

pairs(RT, pch=16, col=as.factor(RT$precipOccurred), panel = panel.smooth)
```

Cloud Cover is the best column to use.

</div>

### Part (b)

Fit three logistic regression models. The first should use lowtemp to predict precipitation. The second should use cloudcover to predict precipitation. The third should use fall to predict precipitation. 

Compare the AIC and residual deviance of each model, which one is "better"?

*Note that each model has a null deviance of 113.14 because they all use the same y-variable and have no missing values in the x-variables. It is wise to check that the null deviance of models you are comparing match up, otherwise, their residual deviances and AIC values aren't directly comparable.*

<div class="YourAnswer">

```{r}
RT.glm <- glm(precipOccurred == 1 ~ lowtemp, data = RT, family = binomial)
summary(RT.glm)

RT2.glm <- glm(precipOccurred == 1 ~ cloudcover, data = RT, family = binomial)
summary(RT2.glm)

RT3.glm <- glm(precipOccurred == 1 ~ fall, data = RT, family = binomial)
summary(RT3.glm)
```

| Predictor Variable | AIC |  Null Deviance | Residual Deviance| $p$-value |
|------------|------------|------------|------------|------------|
| lowtemp | 114.09 | 113.14 | 110.09 | 0.0860 |
| cloudcover | 82.098 | 113.14 | 78.098 | 1.49e-05 |
| fall | 116.79 | 113.14 | 112.79 | 0.5672 |

</div>


### Part (c)

Try every possible 2-variable (no interaction term) logistic regression model that uses both (1) the best variable from Part (b), and (2) each of the other variables in the RT data set, one at a time. Note the AIC of each model. The best two-variable model for this data is has an AIC of 82.75. State the model.

What is the p-value of both variables in this model? Which p-value is not significant?

What is the AIC of this model? Is it better or worse than the "best" one-variable model from Part (b)?

<div class="YourAnswer">

```{r}
RT4.glm <- glm(precipOccurred == 1 ~ cloudcover + lowtemp, data = RT, family = binomial)
summary(RT4.glm)

RT5.glm <- glm(precipOccurred == 1 ~ cloudcover + spring, data = RT, family = binomial)
summary(RT5.glm)

RT6.glm <- glm(precipOccurred == 1 ~ cloudcover + summer, data = RT, family = binomial)
summary(RT6.glm)

RT7.glm <- glm(precipOccurred == 1 ~ cloudcover + fall, data = RT, family = binomial)
summary(RT7.glm)

RT8.glm <- glm(precipOccurred == 1 ~ cloudcover + weekday, data = RT, family = binomial)
summary(RT8.glm)
```

| Predictor Variable | AIC |  Null Deviance | Residual Deviance| First $p$-value | 2nd variable's $p$-value |
|---------------------|------------|---------------|-----------------|------------|--------------------|
| cloudcover | 82.098 | 113.14 | 78.098 | 1.49e-05 | N/A |
| cloudcover + lowtemp | 84.096 | 113.136 | 78.096 | 2.05e-05 | 0.970539 |
| cloudcover + spring | 83.691 | 113.136 | 77.691 | 1.60e-05 | 0.526 |
| cloudcover + summer | 83.962 | 113.136 | 77.962 | 1.59e-05 | 0.714 |
| cloudcover + fall | 83.89 | 113.136 | 77.89 | 1.53e-05 | 0.651 |
| cloudcover + weekday | 82.753 | 113.136 | 76.753 | 9.89e-06 | 0.252 |

The `cloudcover + weekday` model is worse off, `82.753 `, than the one variable model, `82.098`. Additionally, the weekday variable is not significant, `0.252`.
</div>







<style>

.YourAnswer {
  color: #317eac;
  padding: 10px;
  border-style: solid;
  border-width: 2px;
  border-color: skyblue4;
  border-radius: 5px;
}

</style>

 
 