---
title: "Recalling Words"
output: 
  html_document:
    theme: cerulean
    code_folding: hide
---

<script type="text/javascript">
 function showhide(id) {
    var e = document.getElementById(id);
    e.style.display = (e.style.display == 'block') ? 'none' : 'block';
 }
</script>

```{r, include=FALSE}
library(car)
library(tidyverse)
library(mosaic)
library(pander)
library(ggplot2)
library(readr)
library(stringr)
library(DT)
library(pander)
library(gridExtra) #You may need to run: install.packages("DT") 
```

```{r Cheet Sheet for Peer Reviewer, eval=FALSE, message=FALSE, warning=FALSE}
# Play the chunk above and this one to get the data into your Console
#View(Friendly)
#?Friendly



# Does obtaining more subjects affect the results of this test? We only gathered recall words from ten subjects within each group. How would the results change then?
# How do you determine if this data right-skewed, left-skewed, or heavy-tailed?
# Am I suppose to run this test twice? SFR:Meshed and SFR:Before? Or is running the test Meshed:Before the correct way?
# Why does filtering out the SFR group impact the test statistic?
```


<br />

#### Background

Many teachers and other educators are interested in understanding how to best deliver new content to students. In general, they have two choices of how to do this.

1. The Meshed Approach
    * Deliver new content while simultaneously reviewing previously understood content.

2. The Before Approach
    * Deliver new content after fully reviewing previously understood content.

A study was performed to determine whether the *Meshed* or *Before* approaches to delivering content had any positive benefits on memory recall. 

<div style="padding-left:15px;">

##### <a href="javascript:showhide('uniquename')">The Experiment <span style="font-size:8pt;">(click to view)</span></a>


<div id="uniquename" style="display:none;">

Individuals were seated at a computer and shown a list of words. Words appeared on the screen one at a time, for two seconds each, until all words had been shown (40 total). After all words were shown, they were required to perform a few two-digit mathematical additions (like 15 + 25) for 15 seconds to avoid immediate memory recall of the words. They were then asked to write down as many of the 40 words as they could remember. They were given a maximum of 5.3 minutes to recall words.

The process of showing words and recalling words was repeated four times with the same list of words each time (four chances to get it right). The presentation of the first trial was the same for all treatment conditions. However, trials 2, 3, and 4 were slightly different for each treatment condition.

<div style="padding-left:15px;">

The `SFR` group (the control group) stands for Standard Free Recall. In all four trials the same list of 40 words was presented, in a random order each time.

The `Before` group also used the same 40 words during each trial. However, any words that were correctly recalled in a previous trial were presented first, or *before* the words that were not recalled in the last trial. After all the correct words were presented in random order, the non-recalled words were presented in a random order.

The `Meshed` group also used the same 40 words during each trial. However, words that were correctly recalled in a previous trial were alternated with a missed word during the next presentation order. 

</div>

The data records the number of correctly recalled words (out of the 40 possible) from the fourth trial. Results were obtained for 30 students, 10 in each of the three treatment groups: `SFR`, `Before`, and `Meshed`. 

</div>

##### <a href="javascript:showhide('uniquename2')">The Data <span style="font-size:8pt;">(click to view)</span></a>

<div id="uniquename2" style="display:none;">

The results from the study can be found in the `Friendly` data set in R after loading `library(car)`. 

Click the "Code" button to see the data.


```{r}
datatable(Friendly, options=list(lengthMenu = c(3,10,30)))
```


</div>
</div>

<br />


<!-- Begin writing your analysis below here. -->

<!-- Note that your goal is to use the Friendly data to show whether or not the Meshed or Before methods have any positive benefit on memory recall. -->




















#### Analysis


This analysis will determine whether one of the teaching approaches, `Meshed` or `Before`, are stochastically equal or if one is better to use. This can alternatively be formally stated in the following:

$$
  H_0 : \text{The difference in medians} = 0
$$

$$
  H_a : \text{The difference in medians} \neq 0
$$

<br>

We will determine if the differences are significant or not by

$$
  \alpha = 0.05
$$

To determine a difference, a Wilcoxon Rank Sum Test will be used. It will be assumed that the creators of the experiment found subjects who were representive of the overall population so that we can run this test. Before running it though, we first have to narrow the groups down to two. When running the test, I filtered the control group out as we are unable to combine their results into either approach to create two groups. It would be useful in the future to run an ANOVA or Kruskal-Wallis test to determine the effectiveness of these tests with the control group in mind.



<center>

```{r}
Friendly2 <- filter(Friendly, condition %in% c("Meshed", "Before")) %>%
  droplevels()

boxplot(correct ~ condition, 
        data=Friendly, 
        col="dimgray", 
        main="Which Method of Learning Is Best", 
        xlab="Method of Learning", 
        ylab="Number of Correctly Recalled Words")
stripchart(correct ~ condition, 
           data=Friendly, 
           method="stack", 
           vertical=TRUE, 
           pch=16, 
           col="firebrick", 
           cex=1.25, 
           add=TRUE)

```

</center>

The above graph shows us two things. One, we are able to see the distribution of results by the data points. Note the control group is present *only* to show a visual difference in how the tested approaches differ from it. It's interesting to see how both of these approaches fair better than their randomized controlled counterpart. However, it is too difficult to determine whether the two approaches are different enough from one another. This will come from our Wilcoxon test. To better understand the graph, a summary of the data is listed below:

```{r}
Friendly %>%
  group_by(condition) %>%
  summarise(Min = min(correct), 
            Median = median(correct), 
            Mean = mean(correct), 
            Max = max(correct), 
            SD = sd(correct), 
            `Number of Subjects` = n()) %>%
pander(caption="Summary of Experiment Results")
```

When I ran the Wilcoxon Rank Sum Test for the `Before` and `Meshed` approaches, I found that the difference between the medians of the two approaches were not sufficently different to reject our null hypothesis ($p$ = 0.378 > $\alpha$). 

$$
  H_0 : \text{The difference in medians} = 0
$$
Thus we can determine that whichever approach an educator would like to use can be done so in clear conscience that they are using the best approach. The test results can found in the `Appendix` section.

<br>

**A Word Of Advice**: Since same grouped subjects tied in the number of correct words recalled, an exact $p$-value could not be computed for our test. An approximation of the $p$-value was instead used. While a Wilcoxon test can be trusted for small or abnormal data, the results of this experiment only had ten subjets per group. A suggestion for the future would be to obtain 30 or more subjects for each approach and then run an Independent $t$-Test before statements of this experiment are spread to educators. 












---

#### Appendix {.tabset .tabset-pills .tabset-fade}

##### Hide Info

##### Show Test {.tabset .tabset-fade}

For reasons I don't understand, the Test Statistic when using the original dataset (38) differs from the modified dataset (62). The p-value remains the same for each way you run the test. I have included them both for your consideration. Click on the code buttons to see the differences.

###### Test With Normal Data 


```{r Wilcoxon Test, message=FALSE, warning=FALSE}
# Why did Option 2 not work on the Wilcoxon Test? Because you had a ~ instead of a comma.

pander(wilcox.test(Friendly$correct[Friendly$condition == "Meshed"], Friendly$correct[Friendly$condition == "Before"], mu = 0, alternative = "two.sided", conf.level = 0.95))

```



###### Test With Filtered Data


```{r Wilcoxon With Filtered Data, message=FALSE, warning=FALSE}
Friendly2 <- filter(Friendly, condition %in% c("Meshed", "Before")) %>%
  droplevels()
pander(wilcox.test(correct ~ condition, data = Friendly2, mu = 0, alternative = "two.sided", conf.level = 0.95))
datatable(Friendly2)
```

